# Reinforcement Learning | From Zero to Hero

<div align="center">

**Master Reinforcement Learning from scratch using Deep Q-Networks (DQN)**

*A hands-on beginner's guide to understanding how AI agents learn through trial and error*

</div>

---

## ğŸ“– What is Reinforcement Learning?

Think of teaching a dog tricks:
1. ğŸ• Dog tries different actions
2. ğŸ¦´ Gets treats for good behavior
3. ğŸ§  Learns which actions earn rewards

**Reinforcement Learning** works the same way for AI!

### The Learning Cycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Agent  â”‚ â”€Actionâ”€â†’â”‚ Environment â”‚â”€Rewardâ”€â”€â†’â”‚  Agent  â”‚
â”‚         â”‚â†â”€Stateâ”€â”€â”€â”‚             â”‚          â”‚ Learns  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RL vs Other Machine Learning

| Traditional ML | Reinforcement Learning |
|----------------|------------------------|
| Learns from labeled examples | Learns from trial and error |
| "This is a cat" | "That action got +10 points!" |
| One-shot prediction | Sequential decision making |
| Static dataset | Interactive environment |

> **Core Idea:** The agent discovers the best strategy by trying actions and learning from rewards.

---

##  1. The CartPole Challenge

### ğŸª Imagine This Scenario
You're balancing a broomstick on your palm. It starts tilting left â€” you move your hand left to catch it. Too far? Now move right! That's exactly what our AI learns to do.

![CartPole]()

###  Game Mechanics

| Element | Details |
|---------|---------|
| **ğŸ¯ Goal** | Keep pole upright as long as possible |
| **ğŸ“ State** | 4 numbers: cart position, cart speed, pole angle, pole rotation speed |
| **ğŸ® Actions** | Move left (0) or Move right (1) |
| **ğŸ Reward** | +1 point for every moment pole stays balanced |
| **âŒ Game Over** | Pole tilts >12Â°, cart exits bounds, or 500 steps reached |

### ğŸ’¡ Why Start with CartPole?

| Reason | Benefit |
|--------|---------|
| ğŸ¯ Simple physics | Focus on RL concepts, not game complexity |
| ğŸ“ Small state space | Only 4 numbers to track |
| âš¡ Fast training | See results in minutes, not hours |
| ğŸ“ Educational | Every RL concept appears here |

---

## ğŸ§  2. Deep Q-Network (DQN) Explained

### What's a Q-Value? ğŸ¤”

Imagine you're playing chess. A Q-value tells you: **"How good is this move in this position?"**

```
Q(state, action) = Expected total future score if I take this action
```

**Example:**
- State: Pole tilting left at 5Â°
- Action: Move cart left
- Q-value: 450 (this is a GREAT move!)

###  Traditional vs Deep Q-Learning

<table>
<tr>
<th>Q-Learning (Old School)</th>
<th>Deep Q-Learning (Modern)</th>
</tr>
<tr>
<td>

```
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚Stateâ”‚Left  â”‚Right â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚  1  â”‚ 10.5 â”‚  8.2 â”‚
â”‚  2  â”‚  7.1 â”‚ 12.3 â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
   â†‘ Q-Table
```

</td>
<td>

```
   State (4 numbers)
         â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Neural  â”‚
   â”‚ Network  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
   [Q(left), Q(right)]
```

</td>
</tr>
<tr>
<td>âŒ Can't handle millions of states</td>
<td>âœ… Learns patterns across all states</td>
</tr>
</table>

### ğŸ§® The Bellman Equation (The Secret Sauce)

```
Q(current_state, action) = immediate_reward + Î³ Ã— best_future_Q_value
                            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                 Now           |        Later
                                             Discount
                                              factor
```

**In plain English:**
> "The value of an action = the reward you get NOW + the best you can do LATER"

**Real example:**
- Move cart left now: +1 reward
- Best future if pole stays balanced: +499
- Discount factor (Î³ = 0.99): Care slightly less about distant future
- **Total Q-value: 1 + 0.99 Ã— 499 = 495.01**

---

## ğŸ—ï¸ 3. How DQN Works: The Complete Journey

### ğŸ—ºï¸ The Big Picture

```mermaid
graph TB
    A[ğŸ® Environment] -->|State| B[ğŸ¤– Agent]
    B -->|Action| A
    A -->|Reward| C[ğŸ’¾ Memory Buffer]
    C -->|Random Batch| D[ğŸ§  Neural Network]
    D -->|Q-values| B
    D -->|Learn| E[ğŸ“‰ Loss Function]
    E -->|Update| D
```

### ğŸ“š Learning Steps Breakdown

<details>
<summary><b>ğŸ”¹ Step 1: Environment Setup</b></summary>

```python
import gymnasium as gym
env = gym.make('CartPole-v1')
state = env.reset()  # Start a new game
```

Get the playground ready!

</details>

<details>
<summary><b>ğŸ”¹ Step 2: Understanding What We See</b></summary>

**State = 4 numbers:**
```python
[cart_position, cart_velocity, pole_angle, pole_angular_velocity]
Example: [0.02, -0.05, 0.03, 0.1]
```

- Cart position: -2.4 to 2.4 (left to right)
- Cart velocity: how fast it's moving
- Pole angle: -0.21 to 0.21 radians (~12Â°)
- Pole angular velocity: how fast it's rotating

</details>

<details>
<summary><b>ğŸ”¹ Step 3: Random Agent (The Baseline)</b></summary>

```python
# Let's try random actions
action = random.choice([0, 1])  # Random left or right
```

**Result:** Pole falls in ~20 steps ğŸ“‰

**Lesson:** Random guessing doesn't work. We need intelligence!

</details>

<details>
<summary><b>ğŸ”¹ Step 4: Building the Brain (Neural Network)</b></summary>

```python
Input Layer:  [4 neurons]  â† State
Hidden Layer: [128 neurons] â† Processing
Hidden Layer: [128 neurons] â† More processing
Output Layer: [2 neurons]  â† Q-values for [left, right]
```

**The network learns:** "Given this state, which action has higher value?"

</details>

<details>
<summary><b>ğŸ”¹ Step 5: Exploration vs Exploitation (Îµ-Greedy)</b></summary>

```python
if random() < epsilon:
    action = random_action()  # ğŸ² Explore
else:
    action = best_action()    # ğŸ¯ Exploit
```

| Phase | Epsilon | Behavior |
|-------|---------|----------|
| Early training | 1.0 â†’ 0.5 | Mostly exploring |
| Mid training | 0.5 â†’ 0.1 | Balanced |
| Late training | 0.1 â†’ 0.01 | Mostly exploiting learned policy |

**Why?** Like learning to cook â€” first you experiment (explore), then you perfect your recipe (exploit).

</details>

<details>
<summary><b>ğŸ”¹ Step 6: Memory (Experience Replay Buffer)</b></summary>

```python
memory = []
memory.append((state, action, reward, next_state, done))
```

**Stores experiences like:** "When I was at position 0.5, I moved left, got +1 reward, and ended at position 0.48"

**Benefits:**
- ğŸ”„ Break correlation (don't learn only from consecutive steps)
- ğŸ“š Learn from past mistakes multiple times
- ğŸ¯ Stable training

</details>

<details>
<summary><b>ğŸ”¹ Step 7: Two Brains Are Better Than One (Target Network)</b></summary>

**Problem:** If we update the network while using it as a target, training becomes unstable (chasing a moving target).

**Solution:** Use TWO networks!

| Network | Purpose | Update Frequency |
|---------|---------|------------------|
| ğŸ§  Policy Network | Picks actions | Every step |
| ğŸ¯ Target Network | Provides stable targets | Every 100 steps |

</details>

<details>
<summary><b>ğŸ”¹ Step 8: Learning (Training Loop)</b></summary>

```python
for episode in range(500):
    state = env.reset()
    total_reward = 0
    
    while not done:
        # 1. Pick action
        action = epsilon_greedy(state)
        
        # 2. Take action
        next_state, reward, done = env.step(action)
        
        # 3. Remember
        memory.store(state, action, reward, next_state, done)
        
        # 4. Learn from past
        if len(memory) > batch_size:
            batch = memory.sample()
            loss = compute_loss(batch)
            optimizer.step()
        
        # 5. Update target network occasionally
        if step % 100 == 0:
            target_network.copy(policy_network)
```

</details>

<details>
<summary><b>ğŸ”¹ Step 9: The Loss Function</b></summary>

```python
# What we predicted
predicted_q = network(state)[action]

# What it should have been (Bellman equation)
target_q = reward + gamma * max(target_network(next_state))

# How wrong were we?
loss = (predicted_q - target_q)Â²
```

The network learns by minimizing this error!

</details>

---

## ğŸ“ 4. Project Structure

```
RL_CartPole_1/
â”‚
â”œâ”€â”€ ğŸ“‚ venv/                          # Virtual environment
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ¯ train_cartpole.py         # Main training script - START HERE!
â”‚   â”œâ”€â”€ ğŸ® play.py                   # Watch your trained agent perform
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ§  dqn_model.py              # Neural network architecture
â”‚   â”œâ”€â”€ ğŸ’¾ replay_buffer.py          # Experience replay memory
â”‚   â”œâ”€â”€ ğŸ² policy.py                 # Îµ-greedy action selection
â”‚   â”œâ”€â”€ ğŸ¯ target.py                 # Target network update logic
â”‚   â”œâ”€â”€ ğŸ“ˆ train_step.py             # Single training iteration
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ’¾ cartpole_dqn.pth          # Saved trained model (created after training)
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ testing/
â”‚       â””â”€â”€ test_files.py            # Unit tests
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt               # Python dependencies
â””â”€â”€ ğŸ“– README.md                      # You are here!
```

---

## âš¡ 5. Quick Start

### Prerequisites âœ…

Before you begin, make sure you have:
- ğŸ Python 3.8 or higher ([Download](https://www.python.org/downloads/))
- ğŸ“¦ pip (comes with Python)
- ğŸ’» Git ([Download](https://git-scm.com/))

### Installation Steps

#### 1ï¸âƒ£ Clone This Repository

```bash
git clone https://github.com/Bit-Bard/RL-CartPole.git
cd RL_CartPole_1
```

#### 2ï¸âƒ£ Activate Virtual Environment

**Windows:**
```bash
venv\Scripts\activate
```

**macOS/Linux:**
```bash
source venv/bin/activate
```

You should see `(venv)` appear in your terminal.

#### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

#### 4ï¸âƒ£ Train Your Agent

```bash
python train_cartpole.py
```

**What you'll see:**
```
Episode 0: Reward = 23
Episode 50: Reward = 87
Episode 100: Reward = 156
Episode 200: Reward = 342
Episode 300: Reward = 500 âœ…
...
Model saved as cartpole_dqn.pth
```
**Training takes:** ~5-10 minutes on a regular laptop

#### 5ï¸âƒ£ Watch Your Trained Agent

```bash
python play.py
```

---

**Install command:**
```bash
pip install -r requirements.txt
```

---

## ğŸ“ˆ 6. What to Expect During Training

### Training Progress Timeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Episode    Avg Reward    Epsilon    Status    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    0-50        20-40        1.0      ğŸ” Exploring â”‚
â”‚   50-100       40-100       0.7      ğŸ¯ Learning  â”‚
â”‚  100-200      100-250       0.3      ğŸ“ˆ Improving â”‚
â”‚  200-300      250-450       0.1      ğŸ“ Mastering â”‚
â”‚  300-500      450-500      0.01      âœ… Solved!   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Performance Metrics

**Success Criteria:**
-  Average reward > 475 over 100 episodes
-  Consistently reaches 500 steps
-  Solves in ~300-400 episodes

**If training isn't working:**
- Check if epsilon is decreasing
- Verify replay buffer is filling up
- Ensure target network is updating
- Try adjusting hyperparameters

---

## ğŸ¥ 8. Demo Video

> ğŸ“¹ A recorded gameplay video showcasing the trained agent's performance is included in this repository.

**The trained agent:**
- ğŸ¯ Balances pole for full 500 steps
- âš¡ Reacts quickly to pole movements
- ğŸ§  Makes intelligent decisions
- ğŸ† Achieves perfect score consistently

---

## ğŸ“ 9. Key Concepts You'll Master

### Fundamental RL Concepts

| Concept | What You Learn |
|---------|----------------|
| **ğŸ¯ States** | How to represent what the agent sees |
| **ğŸ® Actions** | Discrete vs continuous action spaces |
| **ğŸ Rewards** | Designing reward functions |
| **ğŸ§  Policies** | How agents decide what to do |
| **ğŸ“Š Value Functions** | Estimating long-term returns |

### DQN-Specific Techniques

- âœ… **Q-Learning**: Core RL algorithm
- âœ… **Function Approximation**: Using neural networks instead of tables
- âœ… **Experience Replay**: Breaking correlation in training data
- âœ… **Target Networks**: Stabilizing the learning process
- âœ… **Îµ-Greedy Exploration**: Balancing exploration vs exploitation
- âœ… **Bellman Equation**: Mathematical foundation of Q-learning
- âœ… **Temporal Difference Learning**: Learning from prediction errors

### Practical Skills

- ğŸ”§ **PyTorch Implementation**: Building and training neural networks
- ğŸ“Š **Debugging RL**: Common pitfalls and how to fix them
- ğŸ“ˆ **Hyperparameter Tuning**: Finding what works
- ğŸ® **Gymnasium API**: Working with RL environments
- ğŸ’¾ **Model Persistence**: Saving and loading trained models

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

**TL;DR:** You can use, modify, and distribute this code freely. Just give credit!

---

## ğŸ’– Credits & Acknowledgments

<div align="center">

**Created with passion by**

### Dhruv Devaliya (Bit-Bard)

[![GitHub](https://img.shields.io/badge/GitHub-Bit--Bard-181717?logo=github&logoColor=white)](https://github.com/Bit-Bard)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-0077B5?logo=linkedin&logoColor=white)](https://www.linkedin.com/in/dhruv-devaliya)
---

**If this project helped you understand Reinforcement Learning:**
- â­ **Star this repository**
- ğŸ”€ **Fork it** and build something amazing
- ğŸ“¢ **Share it** with others learning RL
- ğŸ’¬ **Open issues** with questions or suggestions

---

**Built with â¤ï¸, lots of â˜•, and countless debugging sessions**

*"The only way to learn reinforcement learning is to reinforce your learning"* ğŸ§ 

</div>

---
